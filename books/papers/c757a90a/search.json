[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview of papers",
    "section": "",
    "text": "Summaries of some of the papers I read and a serachable format."
  },
  {
    "objectID": "arch/vit.html",
    "href": "arch/vit.html",
    "title": "1  ViT",
    "section": "",
    "text": "Tip\n\n\n\nThis is an adaptation of transformer models to work with an image data. Transformers lack the inductive biases of CNNs so they need way more data to perform well. Key things for ViT is scaling the dataset size and pre-training.\nUsing huge datasets allows re-learning some of the inductive biases CNNs have. I.e. positional encodings learn image locations even though they are 1D."
  },
  {
    "objectID": "arch/vit.html#key-points",
    "href": "arch/vit.html#key-points",
    "title": "1  ViT",
    "section": "Key Points",
    "text": "Key Points\n\nWhy is it a good idea to pre-train vs. use CNNs directly?\n\nTransformers allow easy multi modalities.\nXLA and other computational infrastructure exists to make transformers very efficient.\n\n\n\nWhat is the difference in the way Transformers process images vs. CNNs?\nTransformers are design to work on sets, so to make it work with image we need to somehow convert images to set. To turn sets into sequences we add positional encodings to our inputs so they will have the notion of order.\nThe way ViT adopt images to be used with transformers is by using 16x16 patches of raw pixels and transforms those with a linear transformation. The transformed vectors are the tokens.\nCNNs have a local view of the image, that is their receptive field starts small and grows as we more layers. On the other hand, transformers can see the whole image. Locality in ViT is in the form of image patches, so each token represents some local area, but the attention layer can see all the patches.\nPositional embeddings do improve performance, but using 1D is pretty much the same as using 2D. So it seems like the position is not really important, but more like the identity of the patch that matters. Patch distance seem to be encoded in the positional embeddings, also 2D structure seem to emerge from a 1D encoding, this is why hand-crafter 2D encoding doesn’t seem to help there.\nAs we go deeper in attention layers, the attention distance grows. That implies that initial layers learn local features and deeper layers have more complex global representations. This is similar to the way CNNs work.\n\n\nViT features vs. CNNS features\nViT features contain semantic features with high spatial resolution. Each attention layers sees full image. CNN features are localized and coarse. Each feature contains global info (e.g. x32 for the last layer of ResNet)\n\n\n\nViT vs. CNN Features"
  },
  {
    "objectID": "arch/vit.html#questions",
    "href": "arch/vit.html#questions",
    "title": "1  ViT",
    "section": "Questions?",
    "text": "Questions?\n\nKeys are the most stable representations of an image (Why?)"
  },
  {
    "objectID": "arch/vit.html#sources",
    "href": "arch/vit.html#sources",
    "title": "1  ViT",
    "section": "Sources",
    "text": "Sources\n\nDosovitskiy et al. (2020)\nAmir et al. (2021)\n\n\n\n\n\nAmir, Shir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. 2021. “Deep ViT Features as Dense Visual Descriptors.” ArXiv abs/2112.05814.\n\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” arXiv. https://doi.org/10.48550/ARXIV.2010.11929."
  },
  {
    "objectID": "ssl/vito.html",
    "href": "ssl/vito.html",
    "title": "2  SSL Video Pre-training",
    "section": "",
    "text": "TL;DR\n\n\n\n\nRandom crops for video datasets need to be smaller because frame variability is larger than imagenet.\nReplace MoCLR’s average-pooling with small mask prediction network and do the pooling with it.\nVideo should be great for pertaining representation for images, you can see objects change shape and oriantation. For some reason representation pertained on video are not as good as representation pertained on ImageNet. This paper tries to explain why this is the case and proposes ways to close this gap."
  },
  {
    "objectID": "ssl/vito.html#method",
    "href": "ssl/vito.html#method",
    "title": "2  SSL Video Pre-training",
    "section": "Method",
    "text": "Method\nMoCLR is the baseline contrastive learning method they compare against. The method produces multiple views of the same image, average-pools the feature map of all the views, and passes the averaged pooled feature through an MLP. The loss forces all views have the same reduced feature (trained with contrastive loss). The training is done with two copies of the same network, one being trained by gradient decent and the other by exponential moving average of the other."
  },
  {
    "objectID": "ssl/vito.html#whats-new-here",
    "href": "ssl/vito.html#whats-new-here",
    "title": "2  SSL Video Pre-training",
    "section": "What’s new here?",
    "text": "What’s new here?\n\nLarger Random Sized Crops\nContrastive learning methods are designed to fit ImageNet like datasets. ImageNet means single object images and lower variability in image content. Lowe variability allows us to get away with aggressive cropping (8% of the original image). In videos (or natural datasets) aggressive cropping can get you a totally different semantic meaning.\n\n\nAttention Pooling\nIn videos we can do temporal augmentation (look at nearby frames as being similar). Here average pooling might not be as good. Instead of doing object tracking or something similar, they predict an attention mask and use it to do the pooling. The masking is done on multiple scales of the network and all pooled vectors get concatenated before going through the final MLP net. It seems that the attention masks learn to focus on similar stable features in the image pair. \n\n\nCurate dataset to match imagenet\nTasks that we use to measure the quality of a representation are imagenet biased so they do some curation to match distribution of videos to that of imagenet images (by running inference on frames and looking for imagenet categories)."
  },
  {
    "objectID": "ssl/vito.html#sources",
    "href": "ssl/vito.html#sources",
    "title": "2  SSL Video Pre-training",
    "section": "Sources",
    "text": "Sources\n\nParthasarathy et al. (2022)\n\n\n\n\n\nParthasarathy, Nikhil, S. M. Ali Eslami, João Carreira, and Olivier J. H’enaff. 2022. “Self-Supervised Video Pretraining Yields Strong Image Representations.” In. https://www.semanticscholar.org/paper/Self-supervised-video-pretraining-yields-strong-Parthasarathy-Eslami/c95527eed7758904823eb43300044fbd0cb1881c."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Amir, Shir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. 2021.\n“Deep ViT Features as Dense Visual Descriptors.”\nArXiv abs/2112.05814.\n\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al.\n2020. “An Image Is Worth 16x16 Words: Transformers for Image\nRecognition at Scale.” arXiv. https://doi.org/10.48550/ARXIV.2010.11929.\n\n\nParthasarathy, Nikhil, S. M. Ali Eslami, João Carreira, and Olivier J.\nH’enaff. 2022. “Self-Supervised Video Pretraining Yields Strong\nImage Representations.” In. https://www.semanticscholar.org/paper/Self-supervised-video-pretraining-yields-strong-Parthasarathy-Eslami/c95527eed7758904823eb43300044fbd0cb1881c."
  }
]